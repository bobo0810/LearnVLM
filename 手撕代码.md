# Pytorch语法

- 矩阵乘法： @与`torch.matmul`等价
- 开根号：`x ** 0.5`   x的y次方
- 维度交换：`x.permute(0, 2, 1)`   即维度[0,1,2]->[0,2,1]
- 返回内存连续的新Tensor： `x.permute(0, 2, 1).contiguous()`    防止维度交换完之后可能内存不连续

# 手撕代码

## 1. 手写梯度下降 

求x-2=0的解

- 步骤1：定义损失函数、梯度方程

  1. 目标函数`f(x)=x-2`

  2. 损失函数  采用均方误差
     $$
     L(x) = \frac{1}{2} \cdot (x - 2)^2
     $$

  3. 梯度方程  即 损失函数对x求导
     $$
     \frac{\partial L(x)}{\partial x} = \frac{1}{2} \cdot 2(x - 2) = x - 2
     $$

- 步骤2：初始化参数，即初始值x、学习率、迭代次数、梯度收敛阈值

- 步骤3：迭代更新

```python
x = 1  # 初始值
lr = 0.01 # 学习率
max_iter=1000 # 最大迭代次数
thred=1e-6 # 梯度收敛阈值
for i in range(max_iter):
    gradient = x-2 # 梯度公式
    x = x - lr * gradient  #更新x
    # 判断是否收敛
    if abs(gradient) < thred:
        break
```



## 2. SelfAttention⭐️

<img src="./assets/mha.jpg" alt="img" style="zoom:50%;" />
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$
[self-attention视频教程](https://www.bilibili.com/video/BV19YbFeHETz?buvid=ZB4C53B2BBD061C34F8EB053DCC8271B5A7F&is_story_h5=false&mid=vFwLKO%2BwKIe1OV8Bmn8djA%3D%3D&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=603B0A88-701C-401A-B27C-969378CB9173&share_source=WEIXIN&share_tag=s_i&timestamp=1734798463&unique_k=uoTVCzY&up_id=12420432&spm_id_from=333.788.videopod.sections&vd_source=867a12dbe5f2199cb2f7283321debf90)

### SelfAttention三步骤

1. x映射到QKV  形状不变
2. 计算注意力权重
3. 对V加权

```python
import torch
from torch import nn as nn 
# 公式  softmax(Q @ K.T/sqrt(d) ,dim=-1) @ V

class SelfAttention(nn.Module):
    # 初始化
    def __init__(self,hidden_dim):
        super().__init__()
        self.hidden_dim=hidden_dim
        
        # 初始化映射层
        self.q_proj=nn.Linear(hidden_dim,hidden_dim)
        self.k_proj=nn.Linear(hidden_dim,hidden_dim)
        self.v_proj=nn.Linear(hidden_dim,hidden_dim)
        
    
    def forward(self,x):
        # 1. x映射到QKV,形状不变  [batch,seq_len,hidden_dim]
        Q=self.q_proj(x)
        K=self.k_proj(x)
        V=self.v_proj(x)
        
        # 2. 计算注意力权重 [batch,seq_len,seq_len]
        attention_value= Q @ K.permute(0, 2, 1) / (self.hidden_dim**0.5) # 除以根号d
        attention_value=torch.softmax(attention_value,dim=-1)# 最后一维softmax
        # 3. 对V加权
        output= attention_value @ V
        return output
input=torch.rand(3,2,4)
attn=SelfAttention(hidden_dim=4)
output=attn(input)
print(output.shape)
```

## 3. CrossAttention⭐️

输入q和kv是不同模态、形状不同，其余与SelfAttention完全一致

## 4. MultiHeadAttention ⭐️

[MHA视频手撕教程](https://www.bilibili.com/video/BV19mxdeBEbu/?buvid=ZB4C53B2BBD061C34F8EB053DCC8271B5A7F&is_story_h5=false&mid=vFwLKO%2BwKIe1OV8Bmn8djA%3D%3D&p=1&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=603B0A88-701C-401A-B27C-969378CB9173&share_source=WEIXIN&share_tag=s_i&timestamp=1734798463&unique_k=uoTVCzY&up_id=12420432)

### MHA四步骤

1.  输入映射到QKV，形状不变
2. 拆分多头，每个头单独self-attention
3. 逆向重排列
4. 对输出映射

```python
import math
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    # 初始化
    def __init__(self, head_nums, hidden_dim):
        super().__init__()
        
        self.hidden_dim = hidden_dim 

        self.head_nums = head_nums  # 多头的数量
        self.head_dim = hidden_dim // head_nums # 每个头的维度

        # 初始化映射层
        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)

        # 输出映射层
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x, attention_mask=None):
        batch, seq_len, hidden_dim = x.shape

        # 1. 输入映射到QKV,形状不变[batch, seq_len, hidden_dim]
        Q = self.q_proj(x)
        K = self.k_proj(x)
        V = self.v_proj(x)

        # 2. 拆分多头，每个头单独self-attention
        # [batch,seq_len,hidden_dim]->[batch,seq_len,head_nums,head_dim]-> [batch,head_nums,seq_len,head_dim]
        Q = Q.view(batch, seq_len, self.head_nums, self.head_dim).permute(0, 2, 1, 3)
        K = K.view(batch, seq_len, self.head_nums, self.head_dim).permute(0, 2, 1, 3)
        V = V.view(batch, seq_len, self.head_nums, self.head_dim).permute(0, 2, 1, 3)
        
        attention_value = Q @ K.permute(0,1,3,2) / (self.head_dim ** 0.5)
        if attention_mask is not None:
            attention_value = attention_value.masked_fill(
                attention_mask == 0, float("-inf")
            )
        attention_value = torch.softmax(attention_value, dim=-1)
        output = attention_value @ V

        # 3. 逆向重排列： [batch, seq_len, hidden_dim]   contiguous() 返回一个新的内存连续的Tensor
        output = output.permute(0, 2, 1, 3).contiguous().view(batch, seq_len, self.hidden_dim)

        # 4.对输出映射
        output = self.output_proj(output)
        return output


if __name__ == "__main__":
    # 模拟输入
    input=torch.rand(2,3,4)
    x = torch.randn(batch, seq_len, hidden_dim)
    attention_mask = torch.ones(batch, 1, 1, seq_len)  # 简单全1 mask

    # 实例化
    attn=MultiHeadAttention(hidden_dim=4,head_nums=2)
    output = attn(x, attention_mask)
    print(output.shape)  # 输出: (batch, seq_len, hidden_dim)
```