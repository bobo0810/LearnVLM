### 手写梯度下降 

求x-2=0的解,损失函数用 均方误差

- 步骤1：定义损失函数和梯度       损失函数是均方误差f(x)=(x - 2)²，则梯度（导数）为2(x-2)
- 步骤2：初始化参数，即初始值、学习率、迭代次数
- 步骤3：迭代更新   x_n+1 = x_n - α*梯度

```python
def gradient_descent(x0, lr, epsilon=1e-6, max_iter=1000):
    x = x0
    for i in range(max_iter):
        grad = 4 * x * (x**2 - 2)  # 步骤 1:  f(x) = (x - 2)^2 。然后，计算函数的梯度（导数）：步骤 2: 初始化参
        x_new = x - lr * grad
        if abs(x_new - x) < epsilon:
            break
        x = x_new
    return x
x0 = 1  # 初始值
lr = 0.01 # 学习率
solution = gradient_descent(x0, lr)
```



### SelfAttention / CrossAttention



### [MHA：多头自注意力机制MultiHeadAttention(MHA)](https://www.xiaohongshu.com/explore/67598fc90000000007008874?xsec_token=AB7maWSM9adOiGJYZycABX3lsuRSnFRB6VTV9UN9vlS40=&xsec_source=pc_user)✅

https://www.bilibili.com/video/BV19mxdeBEbu/?buvid=ZB4C53B2BBD061C34F8EB053DCC8271B5A7F&is_story_h5=false&mid=vFwLKO%2BwKIe1OV8Bmn8djA%3D%3D&p=1&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=603B0A88-701C-401A-B27C-969378CB9173&share_source=WEIXIN&share_tag=s_i&timestamp=1734798463&unique_k=uoTVCzY&up_id=12420432



Encoder✅ 前馈神经网络FeedForward（FFN）：两个线性层+ReLU